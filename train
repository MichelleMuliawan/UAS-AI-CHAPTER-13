# Importing the environment and brain modules
from environment import Environment
from brain import Brain
from DQN import Dqn
import numpy as np
import matplotlib.pyplot as plt

# Defining the parameters
memSize = 60000
batchSize = 32
learningRate = 0.0001
gamma = 0.9
nLastStates = 4
epsilon = 1.
epsilonDecayRate = 0.0002
minEpsilon = 0.05
filepathToSave = 'model2.h5'

waitTime = 100
# Creating the Environment, the Brain and the Experience Replay Memory
env = Environment(waitTime)
brain = Brain((env.nRows, env.nColumns, nLastStates), learningRate)
model = brain.model
dqn = Dqn(memSize, gamma)

# Making a function that will initialize game states
def resetStates():
    currentState = np.zeros((1, env.nRows, env.nColumns, nLastStates))

    for i in range(nLastStates):
        currentState[:, :, :, i] = env.screenMap

    return currentState, currentState 

# Starting the main loop
epoch = 0
scores = list()
maxNCollected = 0
nCollected = 0.
totNCollected = 0

while True:
    # Resetting the environment and game states
    env.reset()
    currentState, nextState = resetStates()
    epoch += 1
    gameOver = False

    # Starting the second loop in which we play the game and teach our AI
    while not gameOver:

        # Choosing an action to play
        if np.random.rand() < epsilon:
            action = np.random.randint(0, 4)
        else:
            qvalues = model.predict(currentState)[0]
            action = np.argmax(qvalues)

        # Updating the environment
        state, reward, gameOver = env.step(action)

        # Adding new game frame to the next state and deleting the oldest frame from the next state
        state = np.reshape(state, (1, env.nRows, env.nColumns, 1))
        nextState = np.append(nextState, state, axis=3)
        nextState = np.delete(nextState, 0, axis=3)

        # Remembering the transition and training our AI
        dqn.remember([currentState, action, reward, nextState], gameOver)
        inputs, targets = dqn.get_batch(model, batchSize)

        if inputs.shape[0] == 1:
            inputs = np.squeeze(inputs, axis=0)
        inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1], inputs.shape[2], -1))

        # If the input has only one sample, expand it to have a batch dimension
        if inputs.shape[0] == 1:
            inputs = np.expand_dims(inputs, axis=0)

        # Ensure the input shape matches the model's expected input shape
        if inputs.shape[-1] != nLastStates:
            raise ValueError("Input data doesn't match the expected number of states.")
        inputs = np.reshape(inputs, (inputs.shape[0], inputs.shape[1], inputs.shape[2], -1))
        
        model.train_on_batch(inputs, targets)
        model.train_on_batch(inputs[np.newaxis, ...], targets)

        # Checking whether we have collected an apple and updating the current state
        if env.collected:
            nCollected += 1

        currentState = nextState

        # Checking if a record of apples eaten in a round was beaten and if yes then saving the model
        if nCollected > maxNCollected and nCollected > 2:
            maxNCollected = nCollected
            model.save(filepathToSave)

        totNCollected += nCollected
        nCollected = 0

        # Showing the results each 100 games
        if epoch % 100 == 0 and epoch != 0:
            scores.append(totNCollected / 100)
            totNCollected = 0
            plt.plot(scores)
            plt.xlabel('Epoch / 100')
            plt.ylabel('Average Score')
            plt.savefig('stats.png')
            plt.close()

        # Lowering the epsilon
        if epsilon > minEpsilon:
            epsilon -= epsilonDecayRate


        # Showing the results each game

        print('Epoch: ' + str(epoch) + ' Current Best: ' + str(maxNCollected) + ' Epsilon: {:.5f}'.format(epsilon))
